{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2154ba6-9357-420e-9cf5-8957088b767b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "# Note: please don't add any new package, you should solve this problem using only the packages above.\n",
    "#-------------------------------------------------------------------------\n",
    "'''\n",
    "    Problem 1: Linear Regression\n",
    "    In this problem, you will implement the linear regression method based upon gradient descent.\n",
    "    Xw  = y\n",
    "    You could test the correctness of your code by typing `pytest -v test.py` in the terminal.\n",
    "    Note: please don't use any existing package for linear regression problem, implement your own version.\n",
    "'''\n",
    "\n",
    "#--------------------------\n",
    "def compute_Phi(x,p):\n",
    "    '''\n",
    "        Compute the feature matrix Phi of x. We will construct p polynoials, the p features of the data samples. \n",
    "        The features of each sample, is x^0, x^1, x^2 ... x^(p-1)\n",
    "        Input:\n",
    "            x : a vector of samples in one dimensional space, a numpy vector of shape (n,).\n",
    "                Here n is the number of samples.\n",
    "            p : the number of polynomials/features\n",
    "        Output:\n",
    "            Phi: the design/feature matrix of x, a numpy array of shape (n,p).\n",
    "    '''\n",
    "    #########################################\n",
    "    ## INSERT YOUR CODE HERE\n",
    "    #phi1(x) + phi2(x).....phid(x)\n",
    "    #h w (x) = w 0 + w 1 * phi1(x1) + w 2 * phi2(x2) + ... + w d * phid(xd)\n",
    "    \n",
    "\n",
    "\n",
    "    #########################################\n",
    "    return Phi \n",
    "\n",
    "#--------------------------\n",
    "def compute_yhat(Phi, w):\n",
    "    '''\n",
    "        Compute the linear logit value (predicted value) of all data instances. z = <w, x>\n",
    "        Here <w, x> represents the dot product of the two vectors.\n",
    "        Input:\n",
    "            Phi: the feature matrix of all data instance, a float numpy array of shape (n,p). \n",
    "            w: the weights parameter of the linear model, a float numpy array of shape (p,). \n",
    "        Output:\n",
    "            yhat: the logit value (predicted value) of all instances, a float numpy array of shape (n,)\n",
    "        Hint: you could solve this problem using 1 line of code. Though using more lines of code is also okay.\n",
    "    '''\n",
    "    #########################################\n",
    "    ## INSERT YOUR CODE HERE\n",
    "    # y hat = h w (x) = w 0 + w 1 * phi1(x1) + w 2 * phi2(x2) + ... + w d * phid(xd)\n",
    "\n",
    "\n",
    "    #########################################\n",
    "\n",
    "    return yhat\n",
    "\n",
    "    #--------------------------\n",
    "def compute_L(yhat,y):\n",
    "    '''\n",
    "        Compute the loss function: mean squared error. In this function, divide the original mean squared error by 2 for making gradient computation simple. Remember our loss function in the slides.  \n",
    "        Input:\n",
    "            yhat: the predicted sample labels, a numpy vector of shape (n,).\n",
    "            y:  the sample labels, a numpy vector of shape (n,).\n",
    "        Output:\n",
    "            L: the loss value of linear regression, a float scalar.\n",
    "    '''\n",
    "    #########################################\n",
    "    ## INSERT YOUR CODE HERE\n",
    "    # L = J(theta) = (1/2n) * (((yhat1 - y1) + (yhat2 - y2) + ... (yhatn - yn))^2) \n",
    "    \n",
    "\n",
    "\n",
    "    #########################################\n",
    "    return L \n",
    "\n",
    "\n",
    "\n",
    "def compute_dL_dw(y, yhat, Phi):\n",
    "    '''\n",
    "        Compute the gradients of the loss function L with respect to (w.r.t.) the weights w. \n",
    "        Input:\n",
    "            Phi: the feature matrix of all data instances, a float numpy array of shape (n,p). \n",
    "               Here p is the number of features/dimensions.\n",
    "            y: the sample labels, a numpy vector of shape (n,).\n",
    "            yhat: the predicted sample labels, a numpy vector of shape (n,).\n",
    "        Output:\n",
    "            dL_dw: the gradients of the loss function L with respect to the weights w, a numpy float array of shape (p,). \n",
    "\n",
    "    '''\n",
    "    #########################################\n",
    "    ## INSERT YOUR CODE HERE\n",
    "    # dl/dw = dJ(yhat)/dw = (1/n) * ((((ŷ1(1)- y1(1)) + (ŷ2(1) - y2(1)) + ... (ŷd(1) - yd(1))) +\n",
    "    #                              +   (ŷ1(2)- y1(2)) + (ŷ2(2) - y2(2)) + ... (ŷd(2) - yd(2))) + ... \n",
    "    #                              + (((ŷ1(n) - y1(n)) + (ŷ2(n) - y2(n)) + ... (ŷd(n) - yd(n)))\n",
    "    \n",
    "\n",
    "\n",
    "    #########################################\n",
    "    return dL_dw\n",
    "\n",
    "\n",
    "#--------------------------\n",
    "def update_w(w, dL_dw, alpha = 0.001):\n",
    "    '''\n",
    "       Given the instances in the training data, update the weights w using gradient descent.\n",
    "        Input:\n",
    "            w: the current value of the weight vector, a numpy float array of shape (p,).\n",
    "            dL_dw: the gradient of the loss function w.r.t. the weight vector, a numpy float array of shape (p,). \n",
    "            alpha: the step-size parameter of gradient descent, a float scalar.\n",
    "        Output:\n",
    "            w: the updated weight vector, a numpy float array of shape (p,).\n",
    "        Hint: you could solve this problem using 1 line of code\n",
    "    '''\n",
    "    \n",
    "    #########################################\n",
    "    ## INSERT YOUR CODE HERE\n",
    "    # w[j] = w[j] - alpha * dL_dw \n",
    "\n",
    "\n",
    "    #########################################\n",
    "    return w\n",
    "\n",
    "\n",
    "#--------------------------\n",
    "def train(X, Y, alpha=0.001, n_epoch=100):\n",
    "    '''\n",
    "       Given a training dataset, train the linear regression model by iteratively updating the weights w using the gradient descent\n",
    "        We repeat n_epoch passes over all the training instances.\n",
    "        Input:\n",
    "            X: the feature matrix of training instances, a float numpy array of shape (n, p). Here n is the number of data instance in the training set, p is the number of features/dimensions.\n",
    "            Y: the labels of training instance, a numpy integer array of shape (n,). \n",
    "            alpha: the step-size parameter of gradient descent, a float scalar.\n",
    "            n_epoch: the number of passes to go through the training set, an integer scalar.\n",
    "        Output:\n",
    "            w: the weight vector trained on the training set, a numpy float array of shape (p,). \n",
    "    '''\n",
    "\n",
    "    # initialize weights as 0\n",
    "    w = np.array(np.zeros(X.shape[1])).T\n",
    "\n",
    "    for _ in range(n_epoch):\n",
    "\n",
    "    #########################################\n",
    "    ## INSERT YOUR CODE HERE\n",
    "\n",
    "    # Back propagation: compute local gradients \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    # update the parameters w\n",
    "        \n",
    "\n",
    "     #########################################\n",
    "    return w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
